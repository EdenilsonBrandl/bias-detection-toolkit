# Install required libraries (run only once in Colab)
!pip install numpy pandas matplotlib seaborn scikit-learn tensorflow --quiet

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from typing import Tuple

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 1. Generate Synthetic Environmental Data
def generate_environmental_data(n_samples: int = 5000, random_seed: int = 42) -> pd.DataFrame:
    """
    Simulates environmental data affecting photon sensor readings.
    Features:
        - Respiration intensity (0 to 1)
        - Door open state (0 or 1)
        - Weather condition (0: sunny, 1: cloudy, 2: rainy)
    Output:
        - PhotonLevel (numeric)
        - ImpactLevel (0: High Noise, 1: Moderate Noise, 2: Ideal)
    """
    np.random.seed(random_seed)
    records = []

    for _ in range(n_samples):
        respiration = np.random.uniform(0, 1)       # Respiration intensity
        door_open = np.random.choice([0, 1])        # Door state
        weather = np.random.choice([0, 1, 2])       # Weather condition

        # Base photon count with noise
        base_photon = 500 + np.random.normal(0, 20)

        # Environmental effects on photon count
        if respiration > 0.7:
            base_photon += np.random.normal(-15, 5)   # Humidity / micro changes
        if door_open == 1:
            base_photon += np.random.normal(25, 10)   # Additional light influx
        if weather == 0:  # Sunny
            base_photon += np.random.normal(50, 10)
        elif weather == 2:  # Rainy
            base_photon += np.random.normal(-30, 10)

        # Label the impact level on sensor measurement stability
        if base_photon < 480 or base_photon > 580:
            label = 0  # High noise (unstable)
        elif (480 <= base_photon <= 520) or (540 <= base_photon <= 580):
            label = 1  # Moderate noise (usable)
        else:
            label = 2  # Ideal (stable)

        records.append([respiration, door_open, weather, base_photon, label])

    df = pd.DataFrame(records, columns=['Respiration', 'DoorOpen', 'Weather', 'PhotonLevel', 'ImpactLevel'])
    return df

# Generate the dataset
df = generate_environmental_data()
print("Sample of generated data:")
print(df.head())

# 2. Data Visualization
sns.set(style="whitegrid")
sns.pairplot(df, hue="ImpactLevel", diag_kind="kde", corner=True)
plt.suptitle("Feature Distribution by Impact Level", y=1.02)
plt.show()

# 3. Prepare data for Machine Learning models
feature_cols = ['Respiration', 'DoorOpen', 'Weather']
X = df[feature_cols]
y = df['ImpactLevel']

# Split train/test for Random Forest
X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 4. Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)
rf_model.fit(X_train_rf, y_train_rf)

y_pred_rf = rf_model.predict(X_test_rf)

print("Random Forest Classification Report:")
print(classification_report(y_test_rf, y_pred_rf))

# 5. Train Neural Network Classifier
# One-hot encode target
y_cat = tf.keras.utils.to_categorical(y, num_classes=3)

X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(
    X, y_cat, test_size=0.2, random_state=42, stratify=y
)

nn_model = Sequential([
    Dense(32, input_shape=(len(feature_cols),), activation='relu'),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')
])

nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = nn_model.fit(
    X_train_nn, y_train_nn,
    epochs=30,
    batch_size=64,
    validation_split=0.2,
    verbose=2
)

loss, accuracy = nn_model.evaluate(X_test_nn, y_test_nn, verbose=0)
print(f"\nNeural Network Test Accuracy: {accuracy:.4f}")

# 6. Feature Importance Visualization (Random Forest)
importances = rf_model.feature_importances_
sns.barplot(x=feature_cols, y=importances)
plt.title("Feature Importances from Random Forest")
plt.ylabel("Importance")
plt.show()

# 7. Prediction Function for Custom Inputs
def predict_environmental_impact(respiration: float, door_open: int, weather: int) -> None:
    """
    Predicts the impact level on photon sensor measurements using both models.
    Inputs:
        - respiration: float (0 to 1)
        - door_open: int (0 or 1)
        - weather: int (0=sunny,1=cloudy,2=rainy)
    Outputs:
        Prints predictions and confidence scores.
    """
    sample = np.array([[respiration, door_open, weather]])
    rf_pred = rf_model.predict(sample)[0]
    nn_pred_probs = nn_model.predict(sample)[0]
    nn_pred = np.argmax(nn_pred_probs)
    
    impact_mapping = {0: "High Noise (Unstable)", 1: "Moderate Noise (Usable)", 2: "Ideal (Stable)"}

    print(f"Random Forest Prediction: {rf_pred} -> {impact_mapping[rf_pred]}")
    print(f"Neural Network Prediction: {nn_pred} -> {impact_mapping[nn_pred]} (Confidence: {nn_pred_probs[nn_pred]:.2f})")

# Example usage
print("\nExample prediction for: respiration=0.8, door_open=1, weather=0 (sunny)")
predict_environmental_impact(respiration=0.8, door_open=1, weather=0)
