# Install necessary libraries
!pip install numpy pandas matplotlib scikit-learn tensorflow networkx --quiet

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# ----------------------------------------
# Data Generation: Matrix of Interconnected Causes and Subcauses
# ----------------------------------------

def generate_constraint_matrices(n_samples=5000, n_causes=10, n_layers=3, connection_prob=0.3, threshold_ratio=0.5, random_seed=42):
    """
    Generates synthetic samples simulating interconnected cause matrices representing constraints.
    Each sample contains:
      - A matrix encoding cause interdependencies
      - Current state of each cause (0 = resolved, 1 = active)
      - Label indicating if a new constraint emerges (1) or not (0)
    
    Parameters:
    - n_samples: Number of samples to generate
    - n_causes: Number of causes in each matrix (square matrix dimension)
    - n_layers: Number of connection layers (simulating subcauses interactions)
    - connection_prob: Probability that an edge (dependency) exists between causes
    - threshold_ratio: Threshold ratio of hidden interactions sum to trigger new constraint
    - random_seed: Seed for reproducibility
    
    Returns:
    - X: numpy array of shape (n_samples, matrix_size + n_causes) as features
    - y: binary labels array
    """
    np.random.seed(random_seed)
    data = []
    labels = []
    
    for _ in range(n_samples):
        # Initialize empty matrix
        matrix = np.zeros((n_causes, n_causes))
        
        # Populate matrix with layered connections based on connection_prob
        for _ in range(n_layers):
            for i in range(n_causes):
                for j in range(n_causes):
                    if i != j and np.random.rand() < connection_prob:
                        matrix[i, j] = 1
        
        # Current cause states (binary): 1=active, 0=resolved
        cause_state = np.random.randint(0, 2, size=n_causes)
        
        # Calculate hidden interactions: matrix * cause_state vector
        hidden_interactions = matrix @ cause_state
        
        # If sum of hidden interactions above threshold, label as new constraint emerges
        new_constraint = 1 if hidden_interactions.sum() > (threshold_ratio * n_causes) else 0
        
        # Flatten matrix and concatenate with cause state vector as input features
        input_vector = np.concatenate([matrix.flatten(), cause_state])
        
        data.append(input_vector)
        labels.append(new_constraint)
        
    return np.array(data), np.array(labels)


# ----------------------------------------
# Generate Data
# ----------------------------------------

n_causes = 10
X, y = generate_constraint_matrices(n_samples=7000, n_causes=n_causes, n_layers=3, connection_prob=0.25, threshold_ratio=0.5)

print(f"Data shape: {X.shape}, Labels shape: {y.shape}")
print(f"Sample feature vector length: {X.shape[1]}")


# ----------------------------------------
# Data Preprocessing
# ----------------------------------------

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------------------
# Neural Network Model Definition
# ----------------------------------------

model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ----------------------------------------
# Model Training
# ----------------------------------------

history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=2
)

# ----------------------------------------
# Model Evaluation
# ----------------------------------------

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

print("Classification Report on Test Set:")
print(classification_report(y_test, y_pred))

# ----------------------------------------
# Plot Training History
# ----------------------------------------

plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy During Training')
plt.legend()
plt.grid(True)
plt.show()


# ----------------------------------------
# Optional: Visualize One Sample's Cause Matrix as Graph
# ----------------------------------------

def visualize_cause_matrix(sample_index=0):
    matrix_flat = X[sample_index][:n_causes*n_causes]
    cause_state = X[sample_index][n_causes*n_causes:]
    matrix = matrix_flat.reshape(n_causes, n_causes)

    G = nx.DiGraph()
    for i in range(n_causes):
        G.add_node(i, state='Active' if cause_state[i] > 0 else 'Resolved')

    # Add edges based on matrix
    for i in range(n_causes):
        for j in range(n_causes):
            if matrix[i, j] > 0:
                G.add_edge(i, j)

    # Define node colors
    colors = ['red' if G.nodes[n]['state'] == 'Active' else 'green' for n in G.nodes()]

    plt.figure(figsize=(8, 6))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(G, pos, with_labels=True, node_color=colors, arrowsize=15, node_size=600)
    plt.title(f"Constraint Cause Matrix Visualization (Sample {sample_index})")
    plt.show()

# Uncomment to visualize example
# visualize_cause_matrix(sample_index=0)
